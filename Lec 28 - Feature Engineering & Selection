Feature Engineering Methods

1. Handling Missing Values
• Imputation:
    • Fill missing values with mean, median, mode, or other values.

Example:
Feature 1                      Feature 2                       Feature 3
0.1                            0.2                             NaN
0.2                            NaN                             0.6
NaN                            0.6                             0.7

After imputation:
Feature 1                      Feature 2                       Feature 3
0.1                            0.2                             0.65
0.2                            0.4                             0.6
0.15                           0.6                             0.7


2. Encoding Categorical Variables
• One-Hot Encoding:
    • Convert categorical variables into a series of binary columns.
Example:
Color:
• Red
• Blue
• Green

After one-hot encoding:
Color_Red                      Color_Blue                      Color_Green
1                              0                               0
0                              1                               0
0                              0                               1


3. Feature Scaling
• Min-Max Scaling:
    • Scale features to a fixed range, typically [0, 1].
Example:
Feature 1                      Feature 2
10                             100
20                             200
30                             300

After min-max scaling:
Feature 1                      Feature 2
0                              0
0.5                            0.5
1                              1


4. Feature Creation
• Polynomial Features:
    • Create new features by taking polynomial combinations of existing features.
Example:
Feature 1                      Feature 2
1                              2
3                              4
5                              6

After creating polynomial features (degree=2):
Feature 1          Feature 2          Feature 1^2        Feature 2^2          Feature 1*Feature 2        
1                  2                  1                  4                    2
3                  4                  9                  16                   12
5                  6                  25                 36                   30


FEATURE SELECTION METHODS
``````````````````````````
1. Handling Missing Values
Definition:
Handing missing values is crucial in data preprocessing Commen methods include imputation (replacing missing values with the mea medies, mode, or a specific value and 
removing rows or columna with missing values.

import pandas as pd 
from sklearn.impute import SimpleImputer
#Sample data
data = {
'Featurel': [1.0, 2.0, None, 4.0, 5.0], 'Feature2': [2.0, None, 4.0, 5.0, None), 'Feature3': [None, 3.0, 3.5, 4.0, 4.5)
}
df = pd.DataFrame(data)
#Handling missing values
imputer = SimpleImputer (strategy='mean")
df_imputed = pd.DataFrame(imputer.fit_transform(df), columns-df.columns)
print("After Imputation:\n", df_imputed)


2. Encoding Categorical Variables
Definition:
Encoding categorical variables involves converting categorical data into a numerical format that can be used by machine learning algorithms. Common methods include one-hot encoding and label encoding.
Example Table data:
Color
• Red
• Blue
• Green
• Blue
• Red

import pandas as pd
from sklearn.preprocessing import OneHotEncoder
#Sample data
data = {
"Color': ['Red', 'Blue', 'Green', 'Blue', 'Red']\
}
df = pd.DataFrame(data)
#Encoding categorical variables
encoder = OneHotEncoder(sparse=False)
encoded_categories = encoder.fit_transform(df[['Color']])
df_encoded = pd.DataFrame(encoded_categories, columns encoder.get_feature_names_out(['Color']))
df = pd.concat([df, df_encoded), axis=1).drop("Color', axis=1)
print("After One-Hot Encoding:\n", df)


3. Feature Scaling
Definition:
Feature scaling involves normalizing or standardizing features so that they have a similar scale. Common methods include min-max scaling and standardization (z-score normalization).
Example Table Data:
Feature 1                      Feature 2
10                             100
20                             200
30                             300
40                             400
50                             500

import pandas as pd 
from sklearn.preprocessing import MinMaxScaler
#Sample data
data = {
'Featurel': [10, 20, 30, 40, 50],
'Feature2': [100, 200, 300, 400, 500]
}
df = pd.DataFrame(data)
#Feature scaling  
scaler = MinMaxScaler()
df_scaled = pd.DataFrame (scaler.fit_transform(df), columns-df.columns) 
print("After Min-Max Scaling:\n", df_scaled)


4. Feature Creation
Definition
Feature creation involves generating new features froln existing ones to improve the predictive power of machine learning models. Common methods include polynomial features and interaction terms.

import pandas as pd
from sklearn.preprocessing import Polynomial Features
#Sample data
data = {
'Feature1': [1, 2, 3, 4, 5], 'Feature2': [2, 3, 4, 5, 6]
}
df = pd.DataFrame(data)
# Feature creation
poly = PolynomialFeatures(degree-2, include_bias=False)
poly_features = poly.fit_transform(df)
df polypd.DataFrame(poly_features, columns-poly.get_feature_names_out(['Feature1", "Feature2"])) 
print("After Creating Polynomial Features:\n", df poly)


FEATURE SELECTION METHODS
``````````````````````````

1. Variance Thresholding
Explanation:
Variance Thresholding is a simple baseline approach to feature selection. It removes all features whose variance doesn't meet some threshold. By default, it removes all zero-variance features,
ie. features that have the same value in all samples.
Example Table Data
Feature 1        Feature2        Feature3        Constant
1                2               3               1
1                3               4               1
1                4               5               1
1                5               6               1
1                6               7               1
In this table, 'Feature 1' and 'Constant' have low or zero variance.


2. Correlation Matrix Filtering
Explanation:
Correlation Matrix Filtering involves computing the correlation matrix for the features in the dataset and removing one of each pair of features with a high correlation. This helps to reduce redundaricy 
in the data.
Example Table Data
Feature 1        Feature2        Feature3        Feature4
1                2               2               5
2                4               4               6
3                6               6               7
4                8               8               8
5                10              10              9
In this table, 'Feature2' and 'Feature3' are highly correlated with 'Feature1'.


3. Domain Knowledge
Explanation:
Domain knowledge involves using expertise from the specific field or industry to manually select the most relevant features. This method leverages human understanding of which features are likely to be important.
Example Table Data
Age        Salary        Height        Weight
25         50000         5.5           150
30         60000         6.0           160
35         70000         5.8           170
40         80000         5.9           180
45         90000         6.1           190
In this table, 'Age' and 'Salary' might be selected based on domain knowledge indicating their importance.
